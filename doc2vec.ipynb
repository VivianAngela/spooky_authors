{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test  = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shuffle data frame\n",
    "train = train.sample(frac=1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenise and listify phrases\n",
    "train_tokens = train.text.str.replace(r\"\\W\", \" \").str.split()\n",
    "test_tokens = test.text.str.replace(r\"\\W\", \" \").str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16527    [There, seemed, to, be, a, void, and, nothing,...\n",
       "6398     [This, event, caused, many, of, those, who, we...\n",
       "10604    [I, hastily, gave, my, consent, to, this, arra...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16527    HPL\n",
       "6398     MWS\n",
       "10604    MWS\n",
       "Name: author, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.author[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19579"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Train a doc2vec model using labeled sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for i in range(0,len(train_tokens)):\n",
    "    # print(train.author[0])\n",
    "    # print(train_tokens[0])\n",
    "    documents.append( LabeledSentence(words=list(train_tokens[i]), tags=[train.author[i]]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledSentence(words=['This', 'process', 'however', 'afforded', 'me', 'no', 'means', 'of', 'ascertaining', 'the', 'dimensions', 'of', 'my', 'dungeon', 'as', 'I', 'might', 'make', 'its', 'circuit', 'and', 'return', 'to', 'the', 'point', 'whence', 'I', 'set', 'out', 'without', 'being', 'aware', 'of', 'the', 'fact', 'so', 'perfectly', 'uniform', 'seemed', 'the', 'wall'], tags=['EAP']),\n",
       " LabeledSentence(words=['It', 'never', 'once', 'occurred', 'to', 'me', 'that', 'the', 'fumbling', 'might', 'be', 'a', 'mere', 'mistake'], tags=['HPL']),\n",
       " LabeledSentence(words=['In', 'his', 'left', 'hand', 'was', 'a', 'gold', 'snuff', 'box', 'from', 'which', 'as', 'he', 'capered', 'down', 'the', 'hill', 'cutting', 'all', 'manner', 'of', 'fantastic', 'steps', 'he', 'took', 'snuff', 'incessantly', 'with', 'an', 'air', 'of', 'the', 'greatest', 'possible', 'self', 'satisfaction'], tags=['EAP']),\n",
       " LabeledSentence(words=['How', 'lovely', 'is', 'spring', 'As', 'we', 'looked', 'from', 'Windsor', 'Terrace', 'on', 'the', 'sixteen', 'fertile', 'counties', 'spread', 'beneath', 'speckled', 'by', 'happy', 'cottages', 'and', 'wealthier', 'towns', 'all', 'looked', 'as', 'in', 'former', 'years', 'heart', 'cheering', 'and', 'fair'], tags=['MWS']),\n",
       " LabeledSentence(words=['Finding', 'nothing', 'else', 'not', 'even', 'gold', 'the', 'Superintendent', 'abandoned', 'his', 'attempts', 'but', 'a', 'perplexed', 'look', 'occasionally', 'steals', 'over', 'his', 'countenance', 'as', 'he', 'sits', 'thinking', 'at', 'his', 'desk'], tags=['HPL'])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4880140"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the doc2vec model using the labeled sentences\n",
    "# TODO tune the hyperparameters\n",
    "model = Doc2Vec(size=50, window=8, min_count=10, workers=1, seed=0, iter =10)\n",
    "model.build_vocab(documents)\n",
    "model.train(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity checks and superficial evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MWS'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get an example sentence and the corresponding author\n",
    "s = train_tokens[3]\n",
    "train.author[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath speckled by happy cottages and wealthier towns all looked as in former years heart cheering and fair'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the example sentence\n",
    "\" \".join(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a test vector\n",
    "infer_vector = model.infer_vector(s)\n",
    "# get the top 3 most similar document labels, here: the authors\n",
    "similar_documents = model.docvecs.most_similar([infer_vector], topn = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MWS', 0.506929874420166),\n",
       " ('EAP', 0.09175601601600647),\n",
       " ('HPL', -0.014733417890965939)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a quick and dirty evaluation of the model\n",
    "# NB: this is not correct any more as we've now trained the model on all sentences\n",
    "wrong   = 0\n",
    "correct = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent, author in zip(train_tokens[:50], train.author[:50]):\n",
    "    infer_vector = model.infer_vector(sent)\n",
    "    similar_documents = model.docvecs.most_similar([infer_vector], topn = 1)\n",
    "\n",
    "    # compare label vs prediction\n",
    "    if author == similar_documents[0][0]:\n",
    "        correct += 1\n",
    "    elif author != similar_documents:\n",
    "        wrong += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct/50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Predicting the most similar author type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for test_sent in test_tokens:\n",
    "    infer_vector = model.infer_vector(test_sent)\n",
    "    similar_documents = model.docvecs.most_similar([infer_vector], topn = 3)\n",
    "    predictions.append(similar_documents)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the submission to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('MWS', 0.23805780708789825),\n",
       "  ('EAP', 0.07191196829080582),\n",
       "  ('HPL', -0.044287968426942825)],\n",
       " [('EAP', 0.2845698595046997),\n",
       "  ('MWS', 0.11994245648384094),\n",
       "  ('HPL', 0.060638200491666794)]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the predictions by author for the submission\n",
    "predictions_sorted = []\n",
    "for prediction in predictions:\n",
    "    predictions_sorted.append(sorted(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the predictions out of the tuples \n",
    "clean_predictions = []\n",
    "\n",
    "for prediction in predictions_sorted:\n",
    "    predictions_only = []\n",
    "    \n",
    "    for tupel in prediction:\n",
    "        predictions_only.append(tupel[1])\n",
    "\n",
    "    clean_predictions.append(predictions_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.07191196829080582, -0.044287968426942825, 0.23805780708789825],\n",
       " [0.2845698595046997, 0.060638200491666794, 0.11994245648384094],\n",
       " [0.26888197660446167, 0.3387783467769623, 0.1766926646232605],\n",
       " [0.31672346591949463, 0.3342685401439667, -0.07852844893932343],\n",
       " [-0.006688646972179413, -0.039688438177108765, 0.004600008949637413],\n",
       " [0.3323460519313812, 0.26446446776390076, 0.29063814878463745],\n",
       " [-0.0763876661658287, -0.053300946950912476, -0.1335180252790451],\n",
       " [-0.2676931619644165, 0.10478182882070541, 0.002795552834868431],\n",
       " [0.39022552967071533, 0.2022542953491211, -0.142977774143219],\n",
       " [0.21931785345077515, -0.08599946647882462, 0.18940061330795288]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions sorted by author name\n",
    "clean_predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of predictions to array\n",
    "predictions_array = np.array(clean_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Kaggle: replace max of each row with 1, rest with 0\n",
    "preds_array_ones = (predictions_array == predictions_array.max(axis=1)[:,None]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct clean dataframe for Kaggle submission\n",
    "df_preds = pd.DataFrame(preds_array_ones, columns = ['EAP', 'HPL', 'MWS'], index=test.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id02310</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id24541</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id00134</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id27757</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id04081</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         EAP  HPL  MWS\n",
       "id                    \n",
       "id02310    0    0    1\n",
       "id24541    1    0    0\n",
       "id00134    0    1    0\n",
       "id27757    0    1    0\n",
       "id04081    0    0    1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preds.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to file\n",
    "df_preds.to_csv('submissions/doc2vec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
